{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "import numpy as np\n",
    "import math\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import string\n",
    "#nltk.download()\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocklist = [\n",
    "    \"Main_Page\",\n",
    "    \"Help:\",\n",
    "    \"Special:\",\n",
    "    \"Portal:\",\n",
    "    \"Talk:\",\n",
    "    \"Template:\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_pipeline(words, ngram_size=1):\n",
    "    tokens = nltk.tokenize.word_tokenize(words)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = nltk.ngrams(words, ngram_size)\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_wiki_link(link, get_text=False):\n",
    "    r = requests.get(link)\n",
    "    \n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    # jank way to figure out redirect links\n",
    "    real_link = soup.find_all('link', {\"rel\" : \"canonical\"})[0].get(\"href\").split(\"#\")[0]\n",
    "    if real_link == link:\n",
    "        real_link = None\n",
    "\n",
    "    wiki_content_links = set()\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        if link[\"href\"] == \"#cite_ref-1\":\n",
    "            break\n",
    "        clean_link = link[\"href\"]\n",
    "        clean_link = urllib.parse.unquote(clean_link)\n",
    "        if clean_link.startswith(\"/wiki/\"):\n",
    "            wiki_link = clean_link[6:]\n",
    "            if any(x in wiki_link for x in blocklist):\n",
    "                continue\n",
    "            wiki_content_links.add(clean_link)\n",
    "\n",
    "    if get_text:\n",
    "        # todo - clean this text\n",
    "        words = soup.find_all('p')\n",
    "        return wiki_content_links, real_link, words\n",
    "\n",
    "    return wiki_content_links, real_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_wiki_api(link, get_text=False):\n",
    "    S = requests.Session()\n",
    "\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    PARAMS = {\n",
    "        \"action\": \"parse\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"links|properties\",\n",
    "        \"redirects\": \"\"\n",
    "    }\n",
    "    PARAMS[\"page\"] = link\n",
    "    if get_text:\n",
    "        PARAMS[\"prop\"] += \"|wikitext\"\n",
    "\n",
    "    R = S.get(url=URL, params=PARAMS)\n",
    "    DATA = R.json()\n",
    "\n",
    "    if \"parse\" not in DATA.keys():\n",
    "        return None\n",
    "    parse_links = DATA[\"parse\"][\"links\"]\n",
    "    pageid = DATA[\"parse\"][\"pageid\"]\n",
    "    redirect_list = DATA[\"parse\"][\"redirects\"]\n",
    "    if len(redirect_list) > 0:\n",
    "        redirect = redirect_list[0]['to']\n",
    "    else:\n",
    "        redirect = None\n",
    "    links = []\n",
    "    for l in parse_links:\n",
    "        links.append(l['*'])\n",
    "\n",
    "    if get_text:\n",
    "        words = DATA[\"parse\"][\"wikitext\"][\"*\"]\n",
    "        words = nltk_pipeline(words)\n",
    "        return links, pageid, redirect, words\n",
    "    return links, pageid, redirect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserHistory:\n",
    "    def __init__(self, user_history):\n",
    "        self.user_history = user_history\n",
    "        self.already_visited_pages = set() # resolves redirects in user_history\n",
    "\n",
    "        # user_vists is a list of links in chronological order ascending\n",
    "        # user_vists[-1] is the current page\n",
    "        self.outgoing_links = Counter()\n",
    "        #self.ingoing_links = set()\n",
    "        \n",
    "        self.words = Counter()\n",
    "        for link in set(user_history):\n",
    "            out_links, pageid, redirect, words = parse_wiki_api(link, get_text=True)\n",
    "            if redirect is not None:\n",
    "                self.already_visited_pages.add(redirect)\n",
    "            else:\n",
    "                self.already_visited_pages.add(link)\n",
    "\n",
    "            self.words.update(words)\n",
    "            self.outgoing_links.update(set(out_links))\n",
    "            #self.ingoing_links.update(parse_wiki_ingoing(link))\n",
    "\n",
    "        # remove self-loops\n",
    "        for page in self.already_visited_pages:\n",
    "            if page in self.outgoing_links:\n",
    "                del self.outgoing_links[page]\n",
    "        #self.ingoing_links -= already_visited_pages\n",
    "        \n",
    "        outgoing_links_list = list(self.outgoing_links.values())\n",
    "        self.mean = np.average(outgoing_links_list)\n",
    "        self.std = np.std(outgoing_links_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = parse_wiki_api(\"Albert Einstein\", get_text=True)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_prefix(suffix):\n",
    "    # suffix is '/wiki/<article title>'\n",
    "    return \"https://en.wikipedia.org/\"+suffix\n",
    "\n",
    "def wiki_title_from_link(wiki_link):\n",
    "    # wiki_link format: \"https://en.wikipedia.org/wiki/Hamilton–Jacobi–Bellman_equation\"\n",
    "    title_unform = wiki_link[30:]\n",
    "    title = title_unform.replace(\"_\",\" \")\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cache:\n",
    "    def __init__(self, fetch_fn):\n",
    "        self.dict = dict()\n",
    "        self.fetch_fn = fetch_fn\n",
    "\n",
    "    def __call__(self, key, args):\n",
    "        if key in self.dict:\n",
    "            return self.dict[key]\n",
    "        result = self.fetch_fn(args)\n",
    "        self.dict[key] = result\n",
    "        return result\n",
    "\n",
    "def linkcount_fetch(wiki_page):\n",
    "    # this is so damn slow\n",
    "    link = f\"https://linkcount.toolforge.org/api/?page={wiki_page}&project=en.wikipedia.org\"\n",
    "    r = requests.get(link).json()\n",
    "    return r[\"wikilinks\"][\"all\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_link_similarity(user_history, target):\n",
    "    # user_history  \n",
    "    #   incorporate idf (just hyperlinks) -> scrape target/what_links_here (expensive)\n",
    "    #   or sample 10000 pages and count link frequency and store it somewhere else                       \n",
    "    #   incorporate ingoing recommendations\n",
    "    # return score(target | user_history)\n",
    "\n",
    "    # how many times does target appear in self.outgoing_links\n",
    "    z_score = (user_history.outgoing_links[target]-user_history.mean)/user_history.std\n",
    "    return .5 * (math.erf(z_score / 2 ** .5) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_link_text_similarity(user_history, target):\n",
    "    # user_history  \n",
    "    #   incorporate idf (text)\n",
    "    #   incorporate ingoing recommendations\n",
    "    # return score(target | user_history)\n",
    "\n",
    "    # references in div class = reflist\n",
    "    target = nltk_pipeline(target)\n",
    "    words = user_history.words\n",
    "    total = 0\n",
    "    for term in target:\n",
    "        total += words[term]\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_coupling_similarity(user_history, target, cache, doc_freq_cache):\n",
    "    # user_history  \n",
    "    #   need to download target and scrape it's links\n",
    "    # pages are similar if their outgoing (ingoing) links have overlap\n",
    "    if target in cache:\n",
    "        results = cache[target]\n",
    "    else:\n",
    "        results = parse_wiki_api(target, get_text=False)\n",
    "        # TODO swap to MediaWiki API\n",
    "        cache[target] = results\n",
    "    if results is None:\n",
    "        # likely that this link doesn't exist\n",
    "        return None\n",
    "    links, pageid, redirect = results\n",
    "    if redirect is not None and redirect in user_history.already_visited_pages:\n",
    "        # don't recommend this page, a redirected variant was in the user history\n",
    "        # (only way to resolve redirects from outgoing-links is through this api call)\n",
    "        return None\n",
    "\n",
    "    # TODO: implement faster doc freq\n",
    "    #doc_freq = doc_freq_cache()\n",
    "    target_outgoing = Counter(links)\n",
    "\n",
    "    score = 0\n",
    "    doc_len = sum(v for v in user_history.outgoing_links.values())\n",
    "\n",
    "    # BM25 hyperparameters that are untuned\n",
    "    k1 = 0.5\n",
    "    k3 = 0.99\n",
    "    b = 0.9\n",
    "    avg_doc_len = 50 # estimate?\n",
    "    for link, count in target_outgoing.items():\n",
    "        query_count = user_history.outgoing_links[link]\n",
    "        if count == 0 or query_count == 0:\n",
    "            continue\n",
    "        #page_name = link.split(\"/wiki/\")[1]\n",
    "        doc_freq = 100 #doc_freq_cache(page_name, page_name)\n",
    "\n",
    "        norm_qtf = (k3+1)*query_count / (k3 + query_count)\n",
    "        norm_tf = count * (k1 + 1) / (count + k1*((1-b)+b*(doc_len/avg_doc_len)))\n",
    "        tf = norm_tf * norm_qtf\n",
    "\n",
    "        num_links_on_wiki = 1\n",
    "        idf = 1 #np.log(num_links_on_wiki / (doc_freq+1))\n",
    "        score += tf * idf\n",
    "    #union = sum(v for v in target_outgoing.values()) + sum(v for v in user_history.outgoing_links.values())\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_outgoing_scores_baseline(user_history):\n",
    "    # composite score_link_similarity and score_link_text_similarity\n",
    "    # (todo: this filters scores, will do re-ranking with coupling similarity, re-ranking with deeper searches, etc)\n",
    "    weight = 0.5 # to be tuned\n",
    "    outgoing_scores = dict()\n",
    "    for link in user_history.outgoing_links:\n",
    "        link_sim = score_link_similarity(user_history, link)\n",
    "        text_sim = score_link_text_similarity(user_history, link)\n",
    "        outgoing_scores[link] = link_sim + weight * text_sim\n",
    "    return outgoing_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_with_coupling(user_history, baseline_scores, num_rerank):\n",
    "    new_rankings = {k:v for k, v in baseline_scores}\n",
    "    for target, score in baseline_scores[:num_rerank]:\n",
    "        new_score = score_coupling_similarity(user_history, target, cache, doc_freq_cache)\n",
    "        if new_score is None:\n",
    "            continue\n",
    "        new_rankings[target] = new_score*5 + score\n",
    "    return new_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_from_history(user_history):\n",
    "    baseline_scores = compute_outgoing_scores_baseline(user_history)\n",
    "    sorted_baseline_scores = [(k, v) for k, v in sorted(baseline_scores.items(), reverse=True, key=lambda item: item[1])]\n",
    "    final_results = rerank_with_coupling(user_history, sorted_baseline_scores, 8)\n",
    "    sorted_final_results = [k for k, v in sorted(final_results.items(), reverse=True, key=lambda item: item[1])]\n",
    "    return sorted_final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_history(user_history):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = dict()\n",
    "doc_freq_cache = Cache(linkcount_fetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [],
   "source": [
    "link1 = wiki_title_from_link(\"https://en.wikipedia.org/wiki/Hamilton–Jacobi–Bellman_equation\")\n",
    "link2 = wiki_title_from_link(\"https://en.wikipedia.org/wiki/Value_function\")\n",
    "link3 = wiki_title_from_link(\"https://en.wikipedia.org/wiki/Optimal_control\")\n",
    "user_history = UserHistory([link1, link2, link3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [],
   "source": [
    "link1 = wiki_title_from_link(\"https://en.wikipedia.org/wiki/Bitcoin\")\n",
    "link2 = wiki_title_from_link(\"https://en.wikipedia.org/wiki/Lightning_Network\")\n",
    "user_history = UserHistory([link1, link2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pseudospectral optimal control',\n",
       " 'Control theory',\n",
       " 'DIDO (optimal control)',\n",
       " 'Control (optimal control theory)',\n",
       " 'Hamiltonian (control theory)',\n",
       " 'Optimal control theory',\n",
       " 'Talk:Optimal control',\n",
       " 'Stochastic control',\n",
       " 'Control strategy',\n",
       " 'Model Predictive Control',\n",
       " 'Sliding mode control',\n",
       " 'Linear-quadratic-Gaussian control',\n",
       " 'Partial differential equation',\n",
       " 'Elliptic partial differential equation',\n",
       " 'Optimization problem',\n",
       " 'Template:Cite journal',\n",
       " 'Bellman equation',\n",
       " 'Differential equation',\n",
       " 'Indirect utility function',\n",
       " 'Riccati equation',\n",
       " 'Objective function',\n",
       " 'Function (mathematics)',\n",
       " 'Costate equation',\n",
       " 'Boundary-value problem',\n",
       " 'Differentiable function',\n",
       " 'Difference equation',\n",
       " 'Measurable function',\n",
       " 'Lyapunov function',\n",
       " 'Dynamic programming',\n",
       " 'Brachistochrone problem',\n",
       " \"Merton's portfolio problem\",\n",
       " 'Loss function',\n",
       " 'Hamilton–Jacobi equation',\n",
       " 'Variational problem',\n",
       " 'Value (mathematics)',\n",
       " 'Bellman pseudospectral method',\n",
       " 'Discrete time and continuous time',\n",
       " 'Continuous time',\n",
       " 'Trajectory optimization',\n",
       " 'Mathematical optimization',\n",
       " 'Discrete time',\n",
       " 'Hamiltonian system',\n",
       " 'Bequest value',\n",
       " 'Sum-of-squares optimization',\n",
       " 'Cost functional',\n",
       " 'Viscosity solution',\n",
       " 'Dynamical system',\n",
       " 'Gauss pseudospectral method',\n",
       " 'Roger W. H. Sargent',\n",
       " 'Dimitri P. Bertsekas',\n",
       " 'State variable',\n",
       " 'Rudolf E. Kálmán',\n",
       " 'Collocation method',\n",
       " \"Pontryagin's maximum principle\",\n",
       " 'Minimax solution',\n",
       " 'Necessary and sufficient condition',\n",
       " 'Arthur E. Bryson',\n",
       " 'Donald E. Kirk',\n",
       " 'Initial condition',\n",
       " 'Calculus of variations',\n",
       " 'Principle of optimality',\n",
       " 'Sufficient condition',\n",
       " 'Necessary condition',\n",
       " 'Richard Bellman',\n",
       " 'Infinitesimal generator (stochastic processes)',\n",
       " 'Boundary condition',\n",
       " 'Category:Articles needing additional references from April 2018',\n",
       " 'Category:Use dmy dates from April 2020',\n",
       " 'Stochastic',\n",
       " 'Itô calculus',\n",
       " 'Category:Articles with unsourced statements from July 2019',\n",
       " 'Costate variable',\n",
       " 'Category:Wikipedia articles needing clarification from October 2018',\n",
       " 'Constraint (mathematics)',\n",
       " 'Thomas J. Sargent',\n",
       " 'MATLAB',\n",
       " 'Edward J. McShane',\n",
       " 'Optimality criterion',\n",
       " 'Matrix (mathematics)',\n",
       " 'Lev Pontryagin',\n",
       " 'Dimitri Bertsekas',\n",
       " 'Michael G. Crandall',\n",
       " 'Artificial neural network',\n",
       " 'Category:AC with 0 elements',\n",
       " 'Proceedings of the National Academy of Sciences of the United States of America',\n",
       " 'Monetary policy',\n",
       " 'Classical physics',\n",
       " 'I. Michael Ross',\n",
       " 'Linear-quadratic regulator',\n",
       " 'Generalized filtering',\n",
       " 'Operations research',\n",
       " 'Consistent Approximations',\n",
       " 'Fiscal policy',\n",
       " 'Envelope theorem',\n",
       " 'PROPT',\n",
       " 'Morton Kamien',\n",
       " 'John Tsitsiklis',\n",
       " 'Taylor expansion',\n",
       " 'Michael Whinston',\n",
       " 'DNSS point',\n",
       " 'Overtaking criterion',\n",
       " 'Doi (identifier)',\n",
       " 'Wendell Fleming',\n",
       " 'Little-o notation',\n",
       " 'Newton notation',\n",
       " 'Kalman filter',\n",
       " 'PID controller',\n",
       " 'Active inference',\n",
       " 'Evolution',\n",
       " 'Shadow price',\n",
       " 'Yu-Chi Ho',\n",
       " 'SNOPT',\n",
       " 'ArXiv (identifier)',\n",
       " 'Backward induction',\n",
       " 'Multilayer perceptron',\n",
       " 'Pierre-Louis Lions',\n",
       " 'José Scheinkman',\n",
       " 'Utility',\n",
       " 'Scrap',\n",
       " 'GPOPS-II',\n",
       " 'TOMLAB',\n",
       " 'Brachistochrone',\n",
       " 'CasADi',\n",
       " 'ISBN (identifier)',\n",
       " 'S2CID (identifier)',\n",
       " 'OCLC (identifier)',\n",
       " 'Bibcode (identifier)',\n",
       " 'PMID (identifier)',\n",
       " 'Andrei Izmailovich Subbotin',\n",
       " 'Subderivative',\n",
       " 'Wikipedia:Citation needed',\n",
       " 'Andreu Mas-Colell',\n",
       " 'Online algorithm',\n",
       " 'Parameter',\n",
       " 'Lars Ljungqvist',\n",
       " 'Supremum',\n",
       " 'FORTRAN',\n",
       " 'ASTOS',\n",
       " 'Nancy Schwartz',\n",
       " 'Pursuit-evasion',\n",
       " 'JModelica.org',\n",
       " 'Moon',\n",
       " 'Unemployment',\n",
       " 'Controllability',\n",
       " 'Spacecraft',\n",
       " 'Raymond Rishel',\n",
       " 'Economy',\n",
       " 'Lagrange multiplier',\n",
       " 'Digital data',\n",
       " 'David Luenberger',\n",
       " 'PMC (identifier)',\n",
       " 'Help:CS1 errors',\n",
       " 'Smoothness',\n",
       " 'JSTOR (identifier)',\n",
       " 'Wikipedia:Verifiability',\n",
       " 'Wayback Machine',\n",
       " 'Wikipedia:Vagueness',\n",
       " 'ISSN (identifier)',\n",
       " 'Wikipedia:Citing sources',\n",
       " 'Help:Maintenance template removal',\n",
       " 'Help:Referencing for beginners',\n",
       " 'Wikipedia:Please clarify']"
      ]
     },
     "execution_count": 879,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_from_history(user_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7fe94ad52fb42cd2c8480166949ec48c6429ac5eb7e0959789098eed3674765"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 ('ml_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
